# Default values for gretel-inference-llm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

gretelConfig:
  ## Deployment user's API key, used to authenticate to the Gretel API (one of apiKey or apiKeySecretRef is required)
  apiKey: ""
  ## Deployment user's API key set as GRETEL_API_KEY in an existing k8s secret. Used to authenticate
  ## to the Gretel API (one of apiKey or apiKeySecretRef is required)
  apiKeySecretRef: ""
  apiEndpoint: "https://api.gretel.cloud"

gretelInitImage:
  enabled: true
  name: s3-download-inference-model
  repository: amazon/aws-cli
  tag: 2.10.0

gretelLLMImage:
  repository: ghcr.io/huggingface/text-generation-inference
  tag: 2.2.0
  pullPolicy: IfNotPresent

gretelLLMConfig:
  modelName: "mistral-7b"

availableLLMConfigs:
  mistral-7b:
    modelName: "mistral-7b"
    modelId: "mistralai/Mistral-7B-Instruct-v0.3"
    modelBucket: "cloud-api-inference-model-storage-prod"
    modelSize: 15 # GB
    maxTotalTokens: 8192
    maxInputLength: 4096
    maxConcurrentRequests: 64
    trustRemoteCode: false
    maxBatchPrefillTokens: ""
    quantize: ""
    ropeFactor: ""
    ropeScaling: ""
  mistral-nemo-2407:
    modelName: "mistral-nemo-2407"
    modelId: "casperhansen/mistral-nemo-instruct-2407-awq"
    modelBucket: "cloud-api-inference-model-storage-prod"
    modelSize: 25 # GB
    maxTotalTokens: 8192
    maxInputLength: 4096
    maxConcurrentRequests: 64
    trustRemoteCode: false
    maxBatchPrefillTokens: ""
    quantize: "awq"
    ropeFactor: ""
    ropeScaling: ""
  llama31-8b:
    modelName: "llama31-8b"
    modelId: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    modelBucket: "cloud-api-inference-model-storage-prod"
    modelSize: 25 # GB
    maxTotalTokens: 8192
    maxInputLength: 4096
    maxConcurrentRequests: 64
    trustRemoteCode: false
    maxBatchPrefillTokens: ""
    quantize: ""
    ropeFactor: ""
    ropeScaling: ""
  mistral:
    modelName: "mistral"
    modelId: "casperhansen/mistral-nemo-instruct-2407-awq"
    modelBucket: "cloud-api-inference-model-storage-prod"
    modelSize: 25 # GB
    maxTotalTokens: 8192
    maxInputLength: 4096
    maxConcurrentRequests: 64
    trustRemoteCode: false
    maxBatchPrefillTokens: ""
    quantize: "awq"
    ropeFactor: ""
    ropeScaling: ""
  llama:
    modelName: "llama"
    modelId: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    modelBucket: "cloud-api-inference-model-storage-prod"
    modelSize: 25 # GB
    maxTotalTokens: 8192
    maxInputLength: 4096
    maxConcurrentRequests: 64
    trustRemoteCode: false
    maxBatchPrefillTokens: ""
    quantize: ""
    ropeFactor: ""
    ropeScaling: ""
  mock:
    modelName: "mock"
    modelId: "mockId"
    modelBucket: "cloud-api-inference-model-storage-prod"
    modelSize: 2 # GB
    maxTotalTokens: 8192
    maxInputLength: 4096
    maxConcurrentRequests: 64
    trustRemoteCode: false
    maxBatchPrefillTokens: ""
    quantize: ""
    ropeFactor: ""
    ropeScaling: ""

service:
  type: ClusterIP
  port: 3000

resources:
  limits:
    memory: 10Gi
    nvidia.com/gpu: 1
  requests:
    memory: 10Gi
    nvidia.com/gpu: 1

readinessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 60
  periodSeconds: 10

extraVolumes: {}
extraVolumeMounts: {}

tolerations: {}
affinity: {}
nodeSelector: {}
serviceAnnotations: {}
podAnnotations: {}
podLabels: {}

imagePullSecrets: []
podSecurityContext: {}

podDisruptionBudget:
  create: false
  minAvailable: 1
  # only set `minAvailable` XOR `maxUnavailable`
  # maxUnavailable: 1

securityContext:
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  runAsUser: 1001
  runAsGroup: 1001
  readOnlyRootFilesystem: true
